# Run a classification with a single random population with exactly one cell from
# each of a preset number of classes.

# This script should be run typically by sbatch via a job*.sh file
# generated by gen_run_scriptsy.py, and from inside a results
# directory, so that single.py and the input_data folder is is in ../

import os, sys, argparse
from os import path as op

parser = argparse.ArgumentParser()
parser.add_argument("seed",          type=int, help="Random seed to use.")
parser.add_argument("pop",           type=str, help="Which population to use.", choices = ["PN", "O", "L"])
parser.add_argument("n_classes",     type=int, help="Number of classes to use.")
parser.add_argument("label",         type=str, help="Whether to decode identity or category.",     choices = ["identity", "category"])
parser.add_argument("pool_baseline", type=int, help="Whether to pool the baseline bins into one.", choices = [0,1], )
parser.add_argument("wnd_size",      type=int, help="Sliding window size.")
parser.add_argument("shuffle",       type=int, help="Whether to shuffle the data.", choices=[0,1])
parser.add_argument("--bins",        type=str, help='Which bins to use. If provided must be specified as "bin1 bin2 ..." i.e. with quotes. Bins are numbered from 0.')
args = parser.parse_args()

# 1. First we'll read out and process the input arguments.

seed= args.seed; print("seed: {}".format(seed))
pop = args.pop;  print(" pop: {}".format(pop))

n_classes = args.n_classes; print("n_classes: {}".format(n_classes))
if n_classes <= 0:
    raise ValueError("n_classes must be > 0.")

label         = args.label;         print("label: {}".format(label))
pool_baseline = args.pool_baseline; print("pool_baseline: {}".format(pool_baseline))

wnd_size      = args.wnd_size;      print("wnd_size: {}".format(wnd_size))
if wnd_size <= 0:
    raise ValueError("wnd_Size must be > 0.")

shuffle       = args.shuffle;       print("shuffle: {}".format(shuffle))

which_bins = [int(b) for b in args.bins.split(" ")] if args.bins else None; print("which_bins: {}".format(which_bins))
if which_bins:
    if wnd_size != 1:
        raise ValueError("wnd_size must be 1 if specific bins are specified, was {}.".format(wnd_size))
    if pool_baseline:
        raise ValueError("pool_baseline must be 0 if specific bins are specified, was {}.".format(pool_baseline))

# 2. Start loading the required libraries and data.
import pickle
import numpy as np
import time
from datetime import datetime
import pandas

# TimedBlock is a context manager that will label and time a block of code.
class TimedBlock():
    def __init__(self, name):
        self.name = name
        pass

    def __enter__(self):
        print("{}: Started {}.".format(datetime.now(), self.name))
        self.start_time = time.time()

    def __exit__(self, *args):
        print("{}: Finished {} in {} seconds.".format(datetime.now(), self.name, time.time() - self.start_time))

name = "seed{}{}".format(seed, "shuf" if shuffle else "orig")

np.random.seed(seed)
with TimedBlock("{}".format(name.upper())):    
    with TimedBlock("LOAD DATA"):

        input_dir = "../input_data" 

        btoc_data   = pickle.load(open(op.join(input_dir, "btoc.p"),  "rb" ))
        btoc_cells  = btoc_data["cells"]
        btoc_groups = btoc_data["groups"]
        btoc_odours = btoc_data["odours"]
        print("Loaded btoc.p")

        btoc = np.load(op.join(input_dir, "btoc.npy"))
        print("Loaded btoc.npy")

        classes_per_pop = pickle.load(open(op.join(input_dir, "classes_per_pop.p"), "rb"))
        print("Loaded classes_per_pop.p")
        print("Population sizes:")
        for pop, classes in classes_per_pop.items():
            print("{:>2}: {}".format(pop, len(classes)))

        db = pandas.read_csv(op.join(input_dir, "db.csv"))
        print("Loaded db.csv")

    # Make sure the number of classes we asked for is valid.    
    if n_classes > len(classes_per_pop[pop]):
        raise ValueError("n_classes {} was larger than max allowed {} for pop {}.".format(n_classes, len(classes_per_pop[pop]), pop))
    elif n_classes <= 0:
        raise ValueError("n_classes must be > 0, was {}.".format(n_classes))
 
    num_all_bins, num_trials, num_odours, num_cells  = btoc.shape

    # Make sure the specified bins were valid, if specified.
    if which_bins:
        if not all([b >= 0 and b<num_all_bins for b in which_bins]):
            raise ValueError("Some specified bins were out of the valid range [0, {}].".format(num_all_bins-1))

    # 3. In the next chunk of code we transform the original data
    #    by pooling the baseline windows if desired, and computing the
    #    responses in a sliding window.
    
    num_baseline_bins = 10 # Baseline is first 500 ms
    num_response_bins = num_all_bins - num_baseline_bins

    # The number of bins in the output
    num_bins1 = num_response_bins - wnd_size + 1 + (1 if pool_baseline else num_baseline_bins)

    # Initialize the output array.
    btoc1 = (0*btoc)[:num_bins1, ]

    # Pool the baseline if needed. Pooling means summarizing
    # the baseline bins by their mean.
    if pool_baseline:
        btoc1[0, ]   = np.mean(btoc[:num_baseline_bins, ], axis=0)
        starting_bin = num_baseline_bins
    else:
        btoc1[0, ]  = np.mean(btoc[:wnd_size, ], axis = 0)
        starting_bin = wnd_size

    # Now fill in the remaining bins by averaging over a sliding window.
    for i in range(1, num_bins1):
        btoc1[i, ] = np.mean(btoc[starting_bin:(starting_bin + wnd_size), ], axis = 0)
        starting_bin += 1
    
    # Keep btoc as btoc_orig and rename btoc1 to btoc so we don't have to keep using 'btoc1'
    btoc_orig = 1.*btoc
    btoc      = 1.*btoc1
    num_bins  = num_bins1

    # The values of the linear SVM penalty parameter that we're going to use.
    C_vals = [10.**i for i in np.arange(-8,2,1)]

    # Pick the classes that we're going to use.
    which_classes = np.random.choice(classes_per_pop[pop], n_classes, replace=False)
    # Pick the cells we're going to use
    which_cells   = [np.random.choice(db[(db["group"] == pop) & (db["class"] == cl)]["cell"]) for cl in which_classes]

    print("which_classes: {}".format("; ".join(which_classes)))
    print("which_cells:   {}".format("; ".join(which_cells)))
    
    if shuffle:
        with TimedBlock("SHUFFLE"):
            # For each cell, shuffle the odor labels of its responses in each trial and bin
            btoc_shuff = 0*btoc
            for ib in range(btoc.shape[0]): # bins 
                for it in range(btoc.shape[1]): # trials
                    for ic in range(btoc.shape[-1]): # cells
                        btoc_shuff[ib,it,:,ic] = np.random.permutation(btoc[ib,it,:,ic])
            btoc = btoc_shuff

    # 4. Finally, start the computation
    with TimedBlock("COMPUTE"):
        with TimedBlock("LOAD SKLEARN"):
            from sklearn.preprocessing import LabelEncoder
            from sklearn.multiclass import OneVsRestClassifier
            from sklearn.model_selection import cross_val_score, cross_val_predict
            from sklearn.preprocessing import scale
            from sklearn.metrics import confusion_matrix
            from sklearn import svm

        # Grab the subset of the data corresponding to the cells we're going to use
        subset = np.stack([btoc[:,:,:,btoc_cells.index(c)] for c in which_cells], axis=3)

        # Reshape the data into a 3-dimensional array of cells x (odours and trials) x bins.
        cotb   = np.transpose(subset)
        c_ot_b = np.reshape(cotb, (len(which_cells), -1, num_bins), order = "F")
        # c_ot_b is now a 3-dimensional array of dimensions num_cells x (num_odours x num_trials) x num_bins.
        # The second dimension ranges over all odours for the first trial, then all odours for the second trial, etc.

        # The labels are either per odour for identity decoding, or per odour group for category decoding.
        # We expand them out 4 times to accommodate the 4 trials.
        labels = {"identity":list(range(len(btoc_odours)))*4, 
                  "category":list(LabelEncoder().fit(btoc_groups).transform(btoc_groups)) * 4}
        y    = np.array(labels[label])
        acc  = []
        for C in C_vals: # Range over the linear SVM parameter
            with TimedBlock("C = {}".format(C)):
                clf   = OneVsRestClassifier(svm.SVC(kernel='linear', C=C))
                accC  = []
                for b in range(c_ot_b.shape[-1]) if not which_bins else which_bins: # Range over the bins
                    X = c_ot_b[:,:,b].T
                    # Train and evaluate a classifier for this bin
                    accC.append(cross_val_score(clf, X, y, cv = 4))
                acc.append(accC)
        acc = np.array(acc)

    # 5. Summarize by writing some statistics to file, and
    #    generating a plot of showing the time course of the accuracy
    #    using gnuplot.
    
    with TimedBlock("SUMMARIZE"):
        mean_acc = np.mean(acc,axis=-1) # Average over the 4 cross validation runs
        max_mean_acc = np.max(mean_acc) # Compute the peak accuracy.
        # Figure out which bin and C value it occured for
        loc_max_mean_acc = np.where(mean_acc == max_mean_acc)
        ind_best_C = loc_max_mean_acc[0][0]
        best_bin   = loc_max_mean_acc[1][0]
    
        summary = {
            "seed":seed,
            "shuffle":shuffle,
            "wnd_size":wnd_size,
            "pool_baseline":pool_baseline,
            "pop":pop,
            "n_classes":n_classes,
            "label":label,
            "which_classes":which_classes,
            "which_cells":which_cells,
            "which_bins":which_bins if which_bins else [],
            "max_acc":max_mean_acc,
            "best_bin":best_bin,
            "C_vals":C_vals,
            "best_C":C_vals[ind_best_C],
            "y_true":y
            }
    
        for item in summary:
            print("{:>16}: {}".format(item, summary[item]))
   
        with open("{}.summary.txt".format(name), "w") as f_out:
            for k in summary:
                f_out.write("{}: {}\n".format(k, summary[k]))
        print("Wrote summary to {}.summary.txt.".format(name) )

        # Finally we'll use gnuplot to plot the accuracy time course for each C value.
        # We'll write out the accuracy data in rows, one per C value.
        with open("{}.csv".format(name), "w") as f_out:
            for ibin in range(mean_acc.shape[1]):
                f_out.write(",".join([str(a) for a in mean_acc[:,ibin]]) + "\n")
        print("Wrote {}.csv".format(name))

        # Generate the gnuplot command to plot the accuracy curves for each C value overlayed.
        plot_command = "gnuplot -e \"set terminal pdf; unset colorbox; set key outside; set size ratio 0.5; set xlabel 'bin'; set ylabel 'accuracy'; set output '{}.pdf'; set palette rgb 33,13,10; set datafile separator ','; plot ".format(name)
        for i,C in enumerate(C_vals):
            plot_command += "'{}' using 0:{} lt palette frac {} title 'C = {}' with lines{} ".format("{}.csv".format(name) if not i else "", i+1, float(i+1)/len(C_vals), C, "," if i<len(C_vals)-1 else ";")
        plot_command += "\""
        print(plot_command)
        os.system(plot_command)
        print("Wrote {}.pdf".format(name))
                    
    # 6. Save the accuracy data and summary to disk.
    with TimedBlock("SAVE RESULTS"):
        np.save("{}_acc".format(name),  acc)
        pickle.dump(summary, open("{}_summary.p".format(name), "wb"))

# Signal ALLDONE so that we can find abnormal exits more easily.         
print("ALLDONE.")
